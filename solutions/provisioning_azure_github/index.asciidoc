//Category=Provisioning
//Platform=Azure
//Maturity level=Advanced

:toc: macro
toc::[]
:idprefix:
:idseparator: -

image::ms_guild_logo.png[Migration Stage "MS Guild Logo", width=116, height=53, align=right, link="https://forms.office.com/Pages/ResponsePage.aspx?id=Wq6idgCfa0-V7V0z13xNYal7m2EdcFdNsyBBMUiro4NUNllHQTlPNU9QV1JRRjk3TTAwVUJCNThTRSQlQCN0PWcu"]

== Provisioning Solution GitHub Actions
=== Overview

GitHub was recently acquired by Microsoft. Microsoft announced that new features will be first incorporated into GitHub instead of Azure DevOps. This announcements shows the future strategic shift from Microsoft away from Azure DevOps to GitHub. The subset of features for implementing pipelines is called GitHub Actions. A pipeline is also called a workflow. So far GitHub Actions are limited to repositories within GitHub.

* Azure Key Vault for storing secrets/ exchange of settings
+
Key vault secrets can be included by importing them as GitHub secrets as described https://docs.microsoft.com/en-us/azure/developer/github/github-key-vault[here].
* Azure App Configuration
+
--
This service provides settings (key-value pairs) and feature toggles. Native integrations exist for typical application programming languages like .NET/ Java. However native integrations with terraform do not exist and it is also not hardened for sensitive information as key vault. Therefore, it is recommended to use that service as special case for application layer if feature toggles are needed.

Changes at in the repo can be automatically pushed to an App Configuration instance as described  https://docs.microsoft.com/en-us/azure/azure-app-configuration/concept-github-action[here].
--
* Azure AD
+
Azure Active Directory provides the service principal the pipelines run with. The essential values are stored as GitHub secrets. From there you can use them as in your workflows as described https://docs.microsoft.com/en-us/azure/developer/github/connect-from-azure[here].
* Monitoring
+
GitHub Actions generates metrics to check the health pipelines and displays te state in the Azure DevOps portal. GitHub Actions provides the Checks API to output statuses, results, and logs for a workflow which you can use for download as described https://stackoverflow.com/questions/64459114/downloading-github-actions-workflow-logs-using-github-api[here]. An azure service without much overhead would be Azure Automation which allows to store and run scripts for polling.
* Structural elements to model environments

The picture illustrates the setup with the major dependencies:

image::complementing_svcs_github.png[Complementing Services, width=594, height=335]

=== Pattern Details
==== Geting Started

Many aspects influence the setup of the service. Following a top down approach the following decisions have to be made:

* Organizational mapping
+
This yields the structural components to host provisioning which will be detailed in the next chapter. It introduces the possible components and guidelines for its structuring.
* Modelling other outlined aspects across automation, infra/ app code and provisioning

The structural components are organizations, teams and projects (in public beta as of 19.09.2021).

*Organizations* are a group of two or more users that typically mirror realworld organizations. They are administered by Organization members and can contain both repositories and teams.

A *project* is a customizable spreadsheet that integrates with your issues and pull requests on GitHub. You can customize the layout by filtering, sorting, and grouping your issues and PRs. You can also add custom fields to track metadata. Projects are flexible so that your team can work in the way that is best for them.

*Teams* give you the ability to create groups of organization members with read, write, or admin permissions to repositories that belong to the organization. Teams are also central to many of GitHub’s collaborative features, such as team @mentions, which notify appropriate groups of people that you’d like their input or attention. Teams can be both project or subject-matter focused, and can relate to job titles or interest groups within your company as well.

When setting up your GitHub Enterprise instance, the immediate instinct may be to create an Organization for every project or department at your company, leading to many divided groups that function in GitHub as siloes. This may seem like a good way to manage permissions and reduce noise, but it’s not always the ideal strategy. In fact, it is detrimental to cross-team collaboration and can result in administrative headaches down the line. https://resources.github.com/downloads/github-guide-to-organizations.pdf[Common setups] are shown below:

[options="header"]
|=======================
|Setup Type|Company size|Benefits
|Single Org and team|small, potentially medium |ideal for highly collaborative start up
|Singe Org, multiple teams|medium/ small companies with strict security needs|more granular repo access
|Multiple Orgs and multiple teams|Large companys with strict repo access|higher level of separation, best for companys with >500 users
|=======================

==== Remaining goals (Automation Code)

This chapter details how the above conceptual features can be achieved with Azure DevOps pipelines. 

Github only allows a *programming approach* for pipelines. YAML organizes pipelines into a hierarchy jobs and steps (a step is also refered to as action). Steps are the workhorse where activities are implemented. Steps support scripting languages as stated below. They in turn allow to install additional libraries frameworks from third party providers such as terraform (or you use extensions that give you additional task types). The list below highlights a few YAML points you have to be aware of: 

* Passing files/ artefacts between jobs/ pipelines
+
Artefacts can be passed between pipelines by uploading and downloading them from GitHub as shown https://github.com/actions/upload-artifact[here].
* Variables
+
--
Variables can have different scopes.  Two ways are possible to pass variables between steps (https://www.edwardthomson.com/blog/github_actions_15_sharing_data_between_steps.html[set-env], https://stackoverflow.com/questions/58033366/how-to-get-current-branch-within-github-actions[set as output]). Direct support for enum like variables do not yet exist or workarounds are needed as described https://github.community/t/can-action-inputs-be-arrays/16457[here].

Variables can be simple key value pairs only. Values can be string and other basic types such as bool. Strings can contain structured json to accommodate more complex structures as shown below:
```
on:
  push:
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - name: set output
      id: set
      run: |
         echo ::set-output name=json_var::'[{ "name": "test", "client_payload": "111" }, { "name": "test2", "client_payload": "222" }] '
    - name: use output
      run: |
        echo $json_var | jq '.[].name'
      env:
        json_var: ${{ steps.set.outputs.json_var}}
```
--
* Obtaining client secret
+
Scripting languages such as terraform might require the client secret for embedded scripting blocks. Due to the direct encoding as GitHub secrets this is not a problem.
* Triggers
+
You can configure your workflows to run when specific activity on GitHub happens, at a scheduled time, or when an event outside of GitHub occurs as described
https://docs.github.com/en/actions/reference/events-that-trigger-workflows[here].

Implicit Chaining for *orchestration* is possible by using trigger condition. Calling other workflows explicitly is so far only possible with scripting as shown here https://github.community/t/start-a-workflow-from-another-workflow/16829[here].

Orchestrated pipelines must pass data between them. The recommended method is to use key vault. 

*Recreation of resources in short intervals* might cause pipelines to fail. Even if resources are deleted they might still exist in the background (even although soft delete is not applicable). Programming languages can therefore get confused if pipelines recreate things in short intervals. Creating a new resource group can solve the problem since they are part of the tecnical resource id.

As part of the *configuration* GitHub Actions provide the following configuration mechanisms:

* *Workflow* input parameters
+
--
When using the `workflow_call` keyword, you can optionally specify inputs that are passed to the called workflow from the caller workflow. Inputs for reusable workflows are specified with the same format as action inputs.
```
on:
  workflow_call:
    inputs:
      username:
        description: 'A username passed from the caller workflow'
        default: 'john-doe'
        required: false
        type: string
  
jobs:
  print-username:
    runs-on: ubuntu-latest

    steps:
      - name: Print the input name to STDOUT
        run: echo The username is ${{ inputs.username }}
```
--
* *Action* can use variables as input. Outputs (=string) of a step/ job can be used in subsequent steps/ jobs.
* *Environments*
+
--
Environments can hold with protection rules such as manual approval and secrets. A workflow job can reference an environment to use the environment's protection rules and secrets. The environment name can be set dynamically in scripts as shwon https://github.community/t/how-to-set-environment-attribute-dynamically-in-a-workflow/163240/5[here].

GitHub Actions includes a collection of variables called contexts and a similar collection of variables called default environment variables.
Default environment variables exist only on the runner that is executing your job.
Most contexts you can use at any point in your workflow, including when default environment variables would be unavailable. For example, you can use contexts with expressions to perform initial processing before the job is routed to a runner for execution; this allows you to use a context with the conditional if keyword to determine whether a step should run. 

Secrets are encrypted environment variables that you create in an organization, repository, or repository environment. The secrets that you create are available to use in GitHub Actions workflows.
--

*Quality gates* can be enforced as follows:

* Static code analysis:
+
Various tool support exists depending on the programming language such as https://github.com/marketplace/actions/sonarqube-scan#:~:text=SonarQube%20GitHub%20Action&text=SonarQube%20is%20an%20open%2Dsource,vulnerabilities%20on%2020%2B%20programming%20languages[SonarCube].
* Automated tests (Unit, Integration, End-To-End)
+
--
Tests can be included in pipelines via additional libraries and additional previous installment through scripting. The workflow below runs npm tests:
```
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Check out code
      - uses: actions/checkout@v2
      - name: Set up node 
        uses: actions/setup-node@v1
      - name: Install dependencies
        run: npm install
      - name: Run tests 
        run: npm test
```
--
* Manual approval e.g. for production
+
--
GitHub actions allows deployments to named environments. Approvers can then be added as environments protection rules. The terraform apply command below is bound to the environment production:
```
  terraformapply:
    name: 'Terraform Apply'
    needs: [terraform]
    runs-on: ubuntu-latest
    environment: production
```
--

==== Remaining goals (Provisioning)

GitHUb Actions *can integrate* with various external tools. Pipelines can be called from external (https://github.community/t/external-trigger/17447[see here]) and allow calling external tools. Various third party tools can be manually installed or used via extensions.

For *compliance* GitHub provides various settings as described https://docs.github.com/en/actions/learn-github-actions/security-hardening-for-github-actions[here].
Secrets can be configured at the organization, repository, or environment level, and allow you to store sensitive information in GitHub. They should not contain structured content like JSON since they are reacted to avoid display in logs.
You can use the CODEOWNERS feature to control how changes are made to your workflow files. For example, if all your workflow files are stored in `.github/workflows`, you can add this directory to the code owners list, so that any proposed changes to these files will first require approval from a designated reviewer.
You should ensure that untrusted input does not flow directly into workflows, actions, API calls, or anywhere else where they could be interpreted as executable code. In addition, there are other less obvious sources of potentially untrusted input, such as branch names and email addresses, which can be quite flexible in terms of their permitted content. For example, `zzz";echo${IFS}"hello";#` would be a valid branch name. A pull request with title of a"; ls $GITHUB_WORKSPACE" would for instance list the directory if the workflow would be as follows:
```
 - name: Check PR title
        run: |
          title="${{ github.event.pull_request.title }}"
          if [[ $title =~ ^octocat ]]; then
          echo "PR title starts with 'octocat'"
          exit 0
          else
          echo "PR title did not start with 'octocat'"
          exit 1
          fi
```
To help you manage the risk of dangerous patterns as early as possible in the development lifecycle, the GitHub Security Lab has developed CodeQL queries that repository owners can integrate into their CI/CD pipelines. https://github.com/github/codeql-action[This action] runs GitHub's industry-leading semantic code analysis engine, CodeQL, against a repository's source code to find security vulnerabilities. It then automatically uploads the results to GitHub so they can be displayed in the repository's security tab.
Actions can use the GITHUB_TOKEN by accessing it from the github.token context. It's good security practice to set the default permission for the GITHUB_TOKEN to read access only for repository contents.

The following *repository* structure shows a conceptual breakdown that covers most aspects:

* 1. Infra
* 1.1. Infrastructure
* 1.1.1. Other landing zones
+
Represents other areas with shared functionality that are required. Examples are environments for monitoring, the environment containing Azure DevOps, Key Vault settings etc.
* 1.1.2. App Environments
+
Represents the environments where application is deployed to.
* 1.1.2.1. Envs
+
This level contains all infrastructure code for seting up en environment. The split between dev and non-dev leverages cost savings for less performant dev environments e.g. by picking cheaper service configurations or totally different Azure services.
* 1.1.2.1.1. Dev
* 1.1.2.1.2. Non-Dev
* 1.1.2.1.3. Modules
+
Factored out modules for shared reuse. One example is a central module to generate the name for a given module.
* 1.1.2.2. Envs-Mgmt
+
Captures aspects assumed by the chosen programming language such as terraform for managing an environment. This includes for instance the backend creation code.
* 1.2. Pipelines
+
Pipelines for automating infrastrcuture deployment.
* 2. App
* 2.1. Application (Black Box)
* 2.2. Pipelines
+
Pipelines for automating app code deployment.
* 3. Shared
+
Captures shared aspects between infrastructure and application code such as publishing key vault secrets for a pipeline or triggering another pipeline.

=== Variations
==== Possible Other Third Party 

For the following features other tools can be used:

* *Project management* support can be added by using other tools such as Azure DevOps.
* *Artefacts* can be stored also in other systems

==== Migrate from Azure DevOps

Workflows/ Repos can be created by porting from Azure DevOps. Besides Azure DevOps pipelines and Azure DevOps Repos additional features/ settings exist, which need to be considered such as Azure DevOps artefacts. The picture below summarizes the starting point.

image::mig_ado_scope.png[Migration Scope, width=1706, height=806]

The migration approach detailed below tries to fulfill the following constraints:

* No big bang introduction of GitHub Actions but incremental approach
* Minimizing parallel infrastructure in GitHub until migration is fully finished

To account for an incremental approach the pipelines are migrated first and the repo including its settings afterwards. The new GitHub workflow is created on the Azure DevOps side and copied over to GitHub Actions for execution. This helps to minimize parallel infrastructure in GitHub until Azure DevOps repos are fully migrated. The  picture below illustrate the first stage (Placement of remote executioner is just one possible example):

image::mig_ado_stage_mig_pl.png[Migration Stage "Migrate Pipelines", width=1728, height=684]

In the second step the Azure DevOps Repo is moved to GitHub. All related settings are migrated. Not required components such as the remote executioner can be dropped. The  picture below illustrate the second stage:

image::mig_ado_stage_mig_rep.png[Migration Stage "Migrate Repo", width=1671, height=679]

The subsequent paragraphs detail the introduced two steps:

1. Migrate Azure DevOps Pipelines to GitHub workflows
+
--
Azure DevOps Pipeline can call GitHub Workflows. This can be used to recreate GitHub Workflows without adjusting the interface of the Azure DevOps pipelines. https://github.com/samsmithnz/AzurePipelinesToGitHubActionsConverter[Tools] exist that allow at least partially to automatically translate the yaml code.

Azure DevOps Artefacts can be migrated to https://docs.github.com/en/actions/advanced-guides/storing-workflow-data-as-artifacts[GitHub artefacts], that can be also uploaded and downloaded for reuse.
--
2. Migrate Azure DevOps Repos to GitHub
+
--
An Azure DevOps repo can be imported into GitHub as described https://docs.github.com/en/github/importing-your-projects-to-github/importing-source-code-to-github/importing-a-git-repository-using-the-command-line[here].
--


=== When to use

Using GitHub makes sense in the following scenarios:

* You need cloud agnostic pipelines e.g. due to a multi-cloud scenario
* Your code repository is GitHub and absence of projects for project management is not a problem or can be replaced with something else such as Azure DevOps
